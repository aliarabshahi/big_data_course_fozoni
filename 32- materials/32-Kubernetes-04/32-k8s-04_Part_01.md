# Our Last Station (Final Session of K8S)

###  Synopsis of this session

Today we have three steps:

1. See how we can setup a cluster of Spark locally ðŸ’ª
2. See how we can work easily with helm charts if we would like to setup a cluster of Spark by k8s ðŸ’¥
3. Go to [AbrArvan](https://www.arvancloud.ir/fa) dashboard and see how we can deploy a cluster of spark on the cloud using helm charts ðŸš€

### Part two- Bitnami Helm Chart

```bash
$ helm search repo bitnami

$ helm repo add bitnami https://charts.bitnami.com/bitnami

$ helm repo update

$ minikube start --memory 7168 --cpus 4

$ helm install spark oci://registry-1.docker.io/bitnamicharts/spark

ðŸš© # If we do not add the repo we will get an Error: failed to download "bitnami/spark" chart version "9.8.3": no available release found

$ helm upgrade spark bitnami/spark --set worker.replicaCount=3

$ helm pull bitnami/spark
```

#### Do you want to see the UI?

```bash
$ kubectl port-forward --address 0.0.0.0 pod/spark-master-0 8080:8080

$ kubectl port-forward --address 0.0.0.0 pod/spark-worker-0 8081:8080
```

Now go to this address
â–¶ http://localhost:8080 

#### Go inside of the pod

```bash
$ kubectl exec -it spark-master-0 -- /bin/bash
```

Test your local cluster by running this

```bash
$ spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://spark-master-0.spark-headless.default.svc.cluster.local:7077 \
  ./examples/jars/spark-examples_2.12-3.5.1.jar 50
```

#### Go one more step deeper

If on the pod we get

```bash
$ ls --all 
```

we can see that there are some limits on folders. So, copy a python file inside one the white directory as follows

```bash
$ kubectl cp simple.py default/spark-master-0:/opt/bitnami/spark/work
```

Now

```bash
$ spark-submit \
  --master spark://spark-master-0.spark-headless.default.svc.cluster.local:7077 \
  ./work/simple.py
```

To see if your cluster works correctly. 

#### Part three- go on the clouds and enjoy deploying your thoughts ðŸ‘ŒðŸ’ªðŸš€ 

On the cloud everything is the same as above. Just take care of the master address, something like the following url

`spark://spark-master-0.spark-headless.fadeer45fdae540d45a4f139b79498a30c0580e3b6df-kuberenetes-04.svc.cluster.local:7077`

