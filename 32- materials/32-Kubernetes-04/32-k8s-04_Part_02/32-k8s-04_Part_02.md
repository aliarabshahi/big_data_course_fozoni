## Install a cluster of Spark Locally (with two nodes)

#### ðŸ›‘ You should have Java Installed!

#### Download and prepare the local environment

- Go to [Downloads | Apache Spark](http://spark.apache.org/downloads.html) and download the latest version of spark. 

- Go to the Linux Terminal (wsl)

  ```bash
  $ mkdir spark-node
  $ mkdir spark-head 
  $ tar -xzvf spark-3.5.0-bin-hadoop3.gz 
  $ cp -r spark-3.5.0-bin-hadoop3/* spark-head
  $ cp -r spark-3.5.0-bin-hadoop3/* spark-node
  $ rm -R spark-3.5.0-bin-hadoop3
  ```

  ## Start Master and Worker nodes

- Start the `master node`

  ```bash
  $  ./spark-head/sbin/start-master.sh --webui-port 8080
  ```

  - Start the first  `Worker node `

  ```bash
  $ ./spark-node/sbin/start-worker.sh  spark://DESKTOP-3F1FI9I.:7077 --webui-port 8081 -p 9911
  ```

  - Start the second `worker node`

  ```bash
  $ export SPARK_IDENT_STRING=worker2
  
  # SPARK_IDENT_STRING: In the case of Spark, this particular variable is used to identify the role or purpose of the Spark component that is being launched. It is typically used within Spark's configuration scripts to differentiate between different Spark workers or instances.
  
  $ ./spark-node/sbin/start-worker.sh  spark://DESKTOP-3F1FI9I.:7077 --webui-port 8082 -p 9912
  ```

- Check out the JVM processes :

  ```bash
  $ jps -m
  ```

  ### Start Spark Shell

- **Scala Shell (we do not work with this item) :** 

  ```bash
  $ ./spark-head/bin/spark-shell
  ```

- **Python Shell (pyspark- our favored shell ):**

  ```bash
  $ export SPARK_HOME=/home/amin/spark/spark-head
  $ export PATH=$SPARK_HOME/bin:$PATH
  $ source ~/.bashrc
  
  $ ./spark-head/bin/pyspark
  
  >>> data=sc.parallelize(range(0,1000))
  >>> data.filter(lambda x: x%3==0).collect()
  ```

- checkout the Web UI : http://172.17.200.170:4040  

  - `PpySpark` runs a local cluster independent of our running cluster


```bash
>>> exit()
```

### Start Jupyter Notebook

- Set `Environment Variables` :

  ```bash
  $ apt install jupyter
  $ export PYSPARK_DRIVER_PYTHON=jupyter
  
  # By setting PYSPARK_DRIVER_PYTHON to "jupyter", you are instructing PySpark to use Jupyter Notebook or JupyterLab as the driver program.
  
  $ export PYSPARK_DRIVER_PYTHON_OPTS='notebook'
  ```

- Install `findspark` to easily find the `spark cluster`

  ```bash
  $ pip3 install jupyter-core findspark
  
  # findspark current version is 2.0.1
  # jupyter-core latest version is 5.3.1
  ```

- Start `Jupyter Notebook`

  ```bash
  $ jupyter notebook
  ```


### Stop Master/Workers

- **In master node (directory):**

  ```bash 
  $ ./spark-head/sbin/stop-master.sh
  ```

- **In worker nodes (directories) :**

  ```bash
  $ ./spark-node/sbin/stop-worker.sh
  ```


### Check the workers:

```bash
$  kill $(jps | grep Worker | awk '{print $1}')

# awk '{print $1}' extracts the first field (column) of the filtered output. By default, awk splits input lines into fields based on whitespace [Whitespace refers to characters that represent space, but do not display any visible symbols or graphical elements. These characters are typically used to separate words, numbers, or other elements within a text or data structure. Common whitespace characters include spaces, tabs, and line breaks], and $1 refers to the first field. In this case, it extracts the PIDs of the Spark workers.
```

 

